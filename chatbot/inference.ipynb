{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300970d9",
   "metadata": {},
   "source": [
    "# 0. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2b525fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import re, time, copy\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80391259",
   "metadata": {},
   "source": [
    "# 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4b8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    bos_token = '<s>'\n",
    "    eos_token = '</s>'\n",
    "    usr_token = '<usr>'\n",
    "    pad_token = '<pad>'\n",
    "    sys_token = '<sys>'\n",
    "    unk_token = '<unk>'\n",
    "    mask_token = '<mask>'\n",
    "    max_length = 384\n",
    "    max_turns = 6\n",
    "    epochs = 4\n",
    "    batch_size = 8\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    learning_rate = 1e-4\n",
    "    model_name = \"skt/kogpt2-base-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ace0c",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb544ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;usr&gt; 나 운전면허 따야 하는데&lt;sys&gt; 너 따긴 했잖아?연수를 받아야지 너는&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;usr&gt; 내일은 산에 못가시나요?&lt;sys&gt; 산에 못가고 장보러 가야지.휴양림가서 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;usr&gt; 오늘 도서관가서 책 빌려왔음&lt;sys&gt; 오호 도서관도 가나? 어디도서관?&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;usr&gt; 파운데이션 하나 같이 테스트 하러 가자&lt;sys&gt; 오 파운데이션 다 썼어?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;usr&gt; 비긴 어게인 다시 보고 싶다&lt;sys&gt; 아 나는 그거 또보고싶다고!&lt;usr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59523</th>\n",
       "      <td>&lt;usr&gt; 첫째랑 둘째 공주 놀이한다고 ㅋㅋ나 티비 보라는데 ㅋㅋ 볼게없는..ㅠ&lt;s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59524</th>\n",
       "      <td>&lt;usr&gt; 우리도 캠핑 가보자&lt;sys&gt; 옥상테라스에 텐트치면 되지 뭘 어딜가.&lt;us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59525</th>\n",
       "      <td>&lt;usr&gt; 너는 군대간 사람 기다릴 수 있을 거 같아?&lt;sys&gt; 글쎄. 누구냐에 따...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59526</th>\n",
       "      <td>&lt;usr&gt; 나 요즘 좀 우울한 것 같아&lt;sys&gt; 엥? 갑자기 왜? 무슨 일 있어?&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59527</th>\n",
       "      <td>&lt;usr&gt; 커피는 씁쓸한게 자기는 좋아?&lt;sys&gt; 나는 달콤한 커피가 좋아.&lt;usr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59528 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            conversation\n",
       "0      <usr> 나 운전면허 따야 하는데<sys> 너 따긴 했잖아?연수를 받아야지 너는<...\n",
       "1      <usr> 내일은 산에 못가시나요?<sys> 산에 못가고 장보러 가야지.휴양림가서 ...\n",
       "2      <usr> 오늘 도서관가서 책 빌려왔음<sys> 오호 도서관도 가나? 어디도서관?<...\n",
       "3      <usr> 파운데이션 하나 같이 테스트 하러 가자<sys> 오 파운데이션 다 썼어?...\n",
       "4      <usr> 비긴 어게인 다시 보고 싶다<sys> 아 나는 그거 또보고싶다고!<usr...\n",
       "...                                                  ...\n",
       "59523  <usr> 첫째랑 둘째 공주 놀이한다고 ㅋㅋ나 티비 보라는데 ㅋㅋ 볼게없는..ㅠ<s...\n",
       "59524  <usr> 우리도 캠핑 가보자<sys> 옥상테라스에 텐트치면 되지 뭘 어딜가.<us...\n",
       "59525  <usr> 너는 군대간 사람 기다릴 수 있을 거 같아?<sys> 글쎄. 누구냐에 따...\n",
       "59526  <usr> 나 요즘 좀 우울한 것 같아<sys> 엥? 갑자기 왜? 무슨 일 있어?<...\n",
       "59527  <usr> 커피는 씁쓸한게 자기는 좋아?<sys> 나는 달콤한 커피가 좋아.<usr...\n",
       "\n",
       "[59528 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "train_per = 0.95\n",
    "\n",
    "data = pd.read_csv('./data/kakao_preprocess.csv')\n",
    "data_train_val = data.sample(N).reset_index(drop=True)\n",
    "data_train_val = data.reset_index(drop=True)\n",
    "data_train = data_train_val[: int(len(data_train_val) * train_per)]\n",
    "data_val = data_train_val[int(len(data_train_val) * train_per): ].reset_index(drop=True)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba8e4797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;usr&gt;  원트 메가 크루 미션 봤어?&lt;sys&gt; 웅웅 되게 파이팅 넘치던데?&lt;us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;usr&gt; 자전거를 타고 출퇴근 해볼까&lt;sys&gt; 안양에서 서울까지 자전거로?&lt;usr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;usr&gt; 연애할 때 가까워 지기 위해서는 상대방 무엇을 알아야할까?&lt;sys&gt; 상대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;usr&gt; 10월 25일이 독도의 날이란다&lt;sys&gt; 독도의 날? 그런게 있었어?&lt;u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;usr&gt; 면허를 따도 쓸 일이 없네&lt;sys&gt; 장롱면허인지 10년 째&lt;usr&gt; 차를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>&lt;usr&gt; 나 방금 병원 다녀왔어 ㅠㅠ&lt;sys&gt; 병원? 어디 아파ㅠ?&lt;usr&gt; 목이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>&lt;usr&gt; 영화볼 때 주인공 위주로 보는 편이야?&lt;sys&gt; 응 난 아무래도 주인공이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>&lt;usr&gt; 포도가 요즘 맛있더라고요.&lt;sys&gt; 무슨 포도 드셨어요?&lt;usr&gt; 샤인머...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>&lt;usr&gt; 오늘 문화센터 특강한던데 한번 가볼래?&lt;sys&gt; 그거 등록해야하는거 아니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>&lt;usr&gt; 회사에서 점심때 소외되는 느낌이에요.&lt;sys&gt; 왜요? 왕따에요?&lt;usr&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3134 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           conversation\n",
       "0     <usr>  원트 메가 크루 미션 봤어?<sys> 웅웅 되게 파이팅 넘치던데?<us...\n",
       "1     <usr> 자전거를 타고 출퇴근 해볼까<sys> 안양에서 서울까지 자전거로?<usr...\n",
       "2     <usr> 연애할 때 가까워 지기 위해서는 상대방 무엇을 알아야할까?<sys> 상대...\n",
       "3     <usr> 10월 25일이 독도의 날이란다<sys> 독도의 날? 그런게 있었어?<u...\n",
       "4     <usr> 면허를 따도 쓸 일이 없네<sys> 장롱면허인지 10년 째<usr> 차를...\n",
       "...                                                 ...\n",
       "3129  <usr> 나 방금 병원 다녀왔어 ㅠㅠ<sys> 병원? 어디 아파ㅠ?<usr> 목이...\n",
       "3130  <usr> 영화볼 때 주인공 위주로 보는 편이야?<sys> 응 난 아무래도 주인공이...\n",
       "3131  <usr> 포도가 요즘 맛있더라고요.<sys> 무슨 포도 드셨어요?<usr> 샤인머...\n",
       "3132  <usr> 오늘 문화센터 특강한던데 한번 가볼래?<sys> 그거 등록해야하는거 아니...\n",
       "3133  <usr> 회사에서 점심때 소외되는 느낌이에요.<sys> 왜요? 왕따에요?<usr>...\n",
       "\n",
       "[3134 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cfc4b",
   "metadata": {},
   "source": [
    "# 3. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "442fa185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(Config.model_name,\n",
    "            bos_token=Config.bos_token, eos_token=Config.eos_token,\n",
    "            unk_token=Config.unk_token, pad_token=Config.pad_token,\n",
    "            mask_token=Config.mask_token, model_max_length=Config.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29b4450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> </s> <usr> <pad> <sys> <unk> <mask> <d> </d> <unused0> "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.convert_ids_to_tokens(i), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aeb33e",
   "metadata": {},
   "source": [
    "# 4. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02c43e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, Config):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.usr_token = Config.usr_token\n",
    "        self.sys_token = Config.sys_token\n",
    "        self.bos_token = Config.bos_token\n",
    "        self.eos_token = Config.eos_token\n",
    "        self.mask_token = Config.mask_token\n",
    "        self.pad_token = Config.pad_token\n",
    "        self.max_length = Config.max_length\n",
    "        self.max_turns = Config.max_turns\n",
    "        \n",
    "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data['conversation'][idx]\n",
    "        # input_id\n",
    "        input_id = self.tokenizer.encode(self.bos_token + sentence + self.eos_token)\n",
    "        # token_type_id\n",
    "        token_type = []\n",
    "        loop = True\n",
    "        for token_id in input_id:\n",
    "            token = self.tokenizer.convert_ids_to_tokens(token_id)\n",
    "            \n",
    "            if token == self.usr_token: loop=True\n",
    "            elif token == self.sys_token: loop=False\n",
    "                \n",
    "            if loop:\n",
    "                token_type.append(self.usr_token)\n",
    "            else:\n",
    "                token_type.append(self.sys_token)\n",
    "        token_type_id = self.tokenizer.convert_tokens_to_ids(token_type)\n",
    "        # label\n",
    "        start_idx = len(input_id) - list(reversed(input_id)).index(4)\n",
    "        label = [-100] * start_idx + input_id[start_idx: ]\n",
    "        # padding\n",
    "        input_id, token_type_id, label = self.make_padding(input_id, token_type_id, label)\n",
    "        \n",
    "        return input_id, token_type_id, label\n",
    "\n",
    "    def make_padding(self, input_id, token_type_id, label):\n",
    "        left_length = self.max_length - len(input_id)\n",
    "        input_id += [self.tokenizer.pad_token_id] * left_length\n",
    "        token_type_id += [self.tokenizer.pad_token_id] * left_length\n",
    "        label += [-100] * left_length\n",
    "        \n",
    "        return input_id, token_type_id, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77933053",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(data_train, tokenizer, Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf724513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_id\n",
      "[0, 2, 9063, 17970, 36100, 9106, 7991, 14696, 4, 10099, 9106, 6960, 9651, 8162, 7965, 406, 8033, 10007, 18408, 8263, 10099, 7162, 2, 15247, 9081, 10007, 13892, 18408, 9337, 9031, 27511, 389, 4, 9774, 9515, 29247, 14807, 2, 9716, 26861, 9685, 8263, 46651, 4, 9273, 11865, 9078, 7182, 18381, 9285, 7607, 12249, 27076, 2, 37472, 18882, 9098, 7661, 7991, 6872, 7098, 4, 10723, 17970, 9033, 6866, 9266, 9328, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "token_type_id\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "label\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 10723, 17970, 9033, 6866, 9266, 9328, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "input_id, token_type_id, label = train_set[0]\n",
    "print(\"input_id\", input_id, sep='\\n')\n",
    "print(\"token_type_id\", token_type_id, sep='\\n')\n",
    "print(\"label\", label, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "920ae8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    input_ids = [items[0] for items in batch]\n",
    "    token_type_ids = [items[1] for items in batch]\n",
    "    labels = [items[2] for items in batch]\n",
    "    \n",
    "    return torch.LongTensor(input_ids), torch.LongTensor(token_type_ids), \\\n",
    "            torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb29c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=Config.batch_size, num_workers=2,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46286789",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = CustomDataset(data_val, tokenizer, Config)\n",
    "val_dataloader = DataLoader(val_set, batch_size=Config.batch_size, num_workers=2,\n",
    "                            shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2458e1c",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc5bcc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(Config.model_name).to(Config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b652b44",
   "metadata": {},
   "source": [
    "# 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ae03384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self, model, tokenizer, Config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=Config.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.9)\n",
    "        self.usr_token_id = tokenizer.get_vocab()[Config.usr_token]\n",
    "        self.sys_token_id = tokenizer.get_vocab()[Config.sys_token]\n",
    "        self.max_length = Config.max_length\n",
    "        self.max_turns = Config.max_turns\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    \n",
    "    def train(self, epochs, train_dataloader, validation_dataloader=None, save=None):\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n Epoch {epoch+1}/{epochs}\", sep=\"\\n\")\n",
    "            start_time = time.time()\n",
    "            batch_loss = []\n",
    "\n",
    "            for i, batch in enumerate(train_dataloader):\n",
    "                input_ids, token_type_ids, labels = batch        \n",
    "                input_ids, token_type_ids, labels = input_ids.to(Config.device), token_type_ids.to(Config.device),\\\n",
    "                                                    labels.to(Config.device)\n",
    "                outputs = self.model(\n",
    "                    input_ids = input_ids,\n",
    "                    token_type_ids = token_type_ids,\n",
    "                    labels = labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                batch_loss.append(loss.item())\n",
    "                \n",
    "                print(self.status(i+1, len(train_dataloader), time.time()-start_time, np.mean(batch_loss)), end='\\r')\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            self.losses.append(np.mean(batch_loss))\n",
    "            \n",
    "            if validation_dataloader:\n",
    "                val_loss = self.validation(validation_dataloader)\n",
    "                print(self.status(i+1, len(train_dataloader), time.time()-start_time, np.mean(batch_loss)) + \\\n",
    "                      \" | val_loss : %.6f\"%(val_loss), end='\\r')\n",
    "                self.val_losses.append(val_loss)\n",
    "            \n",
    "            if save:\n",
    "                time_zone = datetime.timezone(datetime.timedelta(hours=9))\n",
    "                now = datetime.datetime.now(time_zone)\n",
    "                PATH = now.strftime(f'./check_point/%Y-%m-%d-%Hh-%Mm_epoch_{epoch+1+Config.pre_epochs}_sk_labeling_{Config.labeling_type}.pth')\n",
    "                torch.save(self.model.state_dict(), PATH)\n",
    "\n",
    "    def validation(self, validation_dataloader):\n",
    "        self.model.eval()\n",
    "        self.model.to(Config.device)\n",
    "        batch_loss = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(validation_dataloader):\n",
    "                input_ids, token_type_ids, labels = batch\n",
    "                input_ids, token_type_ids, labels = input_ids.to(Config.device), token_type_ids.to(Config.device),\\\n",
    "                                                    labels.to(Config.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids = input_ids,\n",
    "                    token_type_ids = token_type_ids,\n",
    "                    labels = labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                batch_loss.append(loss.item())\n",
    "            \n",
    "            valid_loss = np.mean(batch_loss)\n",
    "        \n",
    "        return valid_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def status(step, step_len, time, loss):\n",
    "        return f\"step : {step}/{step_len} - {int(time)}s | loss : {loss:.6f} | {step/time:.2f}it/s\"\n",
    "\n",
    "    def save_model(self, PATH=None):\n",
    "        if not PATH:\n",
    "            now = datetime.datetime.now()\n",
    "            now_date = now.strftime('%m%d_%H%M')\n",
    "            PATH = 'models/' + str(now_date) + '_model.pt'\n",
    "        torch.save(self.model.state_dict(), PATH)\n",
    "        print(\"model saved.\")\n",
    "\n",
    "    def load_model(self, PATH):\n",
    "        self.model.load_state_dict(torch.load(PATH))\n",
    "        print(\"model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dd3049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chathuman = ChatBot(model, tokenizer, Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/4\n",
      "step : 50/7441 - 41s | loss : 5.080784 | 1.21it/s\r"
     ]
    }
   ],
   "source": [
    "chathuman.train(Config.epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc255d",
   "metadata": {},
   "source": [
    "# 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'models/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23406f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chathuman.save_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9228079",
   "metadata": {},
   "outputs": [],
   "source": [
    "chathuman = ChatBot(model, tokenizer, Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f939deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chathuman.load_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884766fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = \"\"\n",
    "start=True\n",
    "\n",
    "while True:\n",
    "    _input = input(\"user > \")\n",
    "    \n",
    "    if _input == \"끝\":\n",
    "        break\n",
    "    if _input == \"초기화\":\n",
    "        history = \"\"\n",
    "        continue\n",
    "    if start:\n",
    "        _input_word = tokenizer.bos_token + '<usr>' + _input + '<sys>'\n",
    "        history += _input_word\n",
    "        start = False\n",
    "    else:\n",
    "        _input_word = '<usr>' + _input + '<sys>'\n",
    "        if sum([len(i) for i in history]) + len(_input_word) > 100: history = history[1:]\n",
    "        history += _input_word\n",
    "        \n",
    "    input_ids = tokenizer.encode(history, return_tensors=\"pt\").to(Config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=200,\n",
    "            top_k=3,\n",
    "            top_p=0.92,\n",
    "            num_beams=7,\n",
    "            do_samples=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=1,\n",
    "            temperature=0.4,\n",
    "            max_new_tokens=30,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    gen = tokenizer.decode(gen_ids[0])\n",
    "    try:\n",
    "        generated = gen[gen.rfind(\"<sys>\")+5:gen.index(\"</s>\")]\n",
    "    except:\n",
    "        generated = gen[gen.rfind(\"<sys>\")+5:]\n",
    "    history += generated\n",
    "    \n",
    "    print(f'Chatbot > {generated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a792c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e327c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
