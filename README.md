## <p align ="center"> ğŸ•°ï¸ Aiffel Thon </p>

## <p align ="center"> í”„ë¡œì íŠ¸ : ì¸ê³µì§€ëŠ¥ ë¹„ì„œë¥¼ ë§Œë“¤ì–´ë³´ì! </p> 

### <p align ="center"> ğŸ¤– Team ChatHuman ğŸ¤– </p> 

### <p align ="center"> íŒ€ì› </p>

 <p align ="center"> ğŸ¤¹â€â™‚ï¸ ë°©ìŠ¹ìš± ğŸš´â€â™‚ï¸ êµ¬ë³¸íšŒ ğŸŒï¸â€â™‚ï¸ ì´íƒœí™˜ â›·ï¸ ì¥ë¬¸ê·œ </p>

### <p align ="center"> ì´ í”„ë¡œì íŠ¸ëŠ” 22.12.26 ~ 23.02.08ì¼ê¹Œì§€ ì§„í–‰ë˜ëŠ” ì•„ì´í í†¤ í”„ë¡œì íŠ¸ ì…ë‹ˆë‹¤. </p>
---
 
### ì„¸ë¶€ ì¼ì •

|ìˆœì„œ|ê¸°ê°„|ì„¸ë¶€ ê³„íš|
|:---:|:---:|:---:|
|1ë²ˆ|22.12.26 ~ 22.12.30|íŒ€ì›ë“¤ê³¼ì˜ ê³„íš ì¡°ìœ¨|
|2ë²ˆ|23.01.02 ~ 23.02.03|ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°œë°œ í™˜ê²½ êµ¬ì¶•|
|3ë²ˆ|23.01.09 ~ 23.01.13|ëª¨ë¸ í…ŒìŠ¤íŠ¸|
|4ë²ˆ|23.01.09 ~ 23.02.03|ì›¹í˜ì´ì§€ êµ¬ì¶•|
|5ë²ˆ|23.01.16 ~ 23.01.20|ëª¨ë¸ ì—°êµ¬ ë° ì¸í¼ëŸ°ìŠ¤ ì½”ë“œ ì‘ì„±|
|6ë²ˆ|23.01.16 ~ 23.01.27|ëª¨ë¸ í•™ìŠµ|
|7ë²ˆ|23.01.18 ~ 23.02.03|ëª¨ë¸ ë‹¤ë“¬ê¸°|
|8ë²ˆ|23.02.06 ~ 23.02.07|ë°œí‘œ ì¤€ë¹„|
|9ë²ˆ|23.02.08|ë°œí‘œ|
 
---
### ê°œìš”
- ì¼ìƒìƒí™œì— ë„ì›€ì„ ì£¼ë©° í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì‹œë¦¬, í´ë¡œë²„, ì¹´ì¹´ì˜¤ì™€ ê°™ì€ ì¸ê³µì§€ëŠ¥ ë¹„ì„œì—ê²Œ ë¶€ì¬ëœ ê¸°ëŠ¥ì„ íƒ‘ì¬í•˜ëŠ”ê²ƒì„ ëª©í‘œë¡œ í•¨.
  - ë¶€ì¬ëœ ê¸°ëŠ¥ì´ë¼í•¨ì€ ì¼ìƒëŒ€í™” ê¸°ëŠ¥ì— ì´ˆì ì„ ë§ì¶”ë©° ê·¸ ì™¸ì˜ ì‚¬ì†Œí•œ ê²ƒë“¤ì„ ì¶”ê°€í•´ë³¼ ì˜ˆì •.   
  
- ê¸°ë³¸ì ì¸ í˜•íƒœëŠ” ì±—ë´‡ í˜•íƒœì´ë©° ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¥¼ í†µí•´ ëª…ë ¹ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•  ê²ƒ.

![service](https://github.com/Ukbang/Aiffel_thon/blob/main/images/service.png)

---

### Requirement
> Python 3.9.7
> 
> Transformer 4.11.3
>  
> Numpy 1.21.4
>  
> PyTorch 1.9.1+cu111

---

### Dataset
- AIHub ì—ì„œ ì œê³µí•˜ëŠ” ì£¼ì œë³„ í…ìŠ¤íŠ¸ ì¼ìƒìƒí™œ ë°ì´í„°ì™€ í•œêµ­ì–´ ëŒ€í™” ìš”ì•½ì„ ì´ìš©í•˜ì—¬ ë§Œë“¦.
- ë°ì´í„°í”„ë ˆì„ í˜•íƒœë¡œ í•œ ëŒ€í™”ì˜ ë§ë­‰ì¹˜ë¥¼ Conversation columnìœ¼ë¡œ êµ¬ë¶„í•˜ê³  ê° ëŒ€í™” ê°„ ë°œí™”ìë¥¼ `'<usr>'`, `'<sys>'` í† í°ìœ¼ë¡œ êµ¬ë¶„í•˜ì˜€ìŒ.
- ì•½ 19ë§Œê°œì˜ ëŒ€í™”ë¥¼ ì´ìš©í•¨. 

![data_image](https://github.com/Ukbang/Aiffel_thon/blob/main/images/data_image.jpeg)

---
### ì „ì²˜ë¦¬ ë°©ì‹
- modules/preprocessing.py íŒŒì¼ì˜ clear_sentence í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì²˜ë¦¬.
- #@ì´ë¦„#ì€ make_name í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ëœë¤í•œ ì´ë¦„ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŒ.
- @URL, #@ì‹œìŠ¤í…œ#ì‚¬ì§„#, #@ì´ëª¨í‹°ì½˜#ì€ ì‚­ì œí•˜ì˜€ê³  ë°˜ë³µë˜ëŠ” ã…‹,ã…,ã…œ,ã… ,. ê³¼ ê°™ì€ ë¬¸ìëŠ” 2ê°œë¡œ í†µì¼í•˜ì˜€ìœ¼ë©° ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤í‚¤ ëŠ” ã…‹ã…‹ ë¡œ ë³€ê²½í•˜ì˜€ìŒ.

![make_name](https://github.com/Ukbang/Aiffel_thon/blob/main/images/make_name.jpeg)
![clear_sentence](https://github.com/Ukbang/Aiffel_thon/blob/main/images/clear_sentence.jpeg)

---

### ëª¨ë¸
- ëª¨ë¸ì€ ğŸ¤—Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” gpt2 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ìŒ.
- ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ['skt/kogpt2-base-v2'](https://github.com/SKT-AI/KoGPT2) ì„ ì‚¬ìš©í•¨.   
 
 
<p align ="center"><img src="https://user-images.githubusercontent.com/112140135/216884750-53fb4373-2d9d-4a6a-800b-0062b8b702f5.png" width="800px" height="300px"></p>

---
### í•™ìŠµ ì§„í–‰ê³¼ì • ë¦¬ë”ë³´ë“œ
#### Data type
__Topic = 250000ê°œ__
 
 __Topic+kakao Data = 190000ê°œ ('`<usr>`ë¡œ ëë‚˜ëŠ” ë¬¸êµ¬ ì‚­ì œ', ê¸¸ì´ 256')__
 
__kakao Data = 65000ê°œ__

---

#### Label

__Input = Inputê³¼ Labelì´ ë™ì¼__
 
 
__-100 = ë§ˆì§€ë§‰ `<sys>` ëŒ€í™”ë¥¼ ì œì™¸í•œ -100ì„ ì´ìš©í•œ Masking__

__-100+sys = ëª¨ë“  `<sys>` ëŒ€í™”ë¥¼ ì œì™¸í•œ ëª¨ë“  ëŒ€í™” -100ìœ¼ë¡œ Masking__
 
 
__Shift = Inputì€ `<s>` í† í°ì„ bos_tokenìœ¼ë¡œ ì‚¬ìš©, Labelì€ `</s>`í† í°ì„ eos_tokenìœ¼ë¡œ ì‚¬ìš©í•¨.__

|index|Model|Epochs|Data type|ì§„í–‰ ìƒí™©|ì§„í–‰ ì¼ì‹œ|Label|Loss|Val_Loss|Comment|ì„±ëŠ¥|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|1|skt/kogpt2-base-v2|5|Topic+kakao|Done|2023-01-31|-100|4.290 -> 3.797 -> 3.340 -> 2.803 -> 2.195|3.821 -> 3.759 -> 3.804 -> 3.938 -> 4.143|ë‹¨ë‹µí˜•ì´ê³  ëŒ€í™”ê°€ ì˜ ì´ë£¨ì–´ ì§€ì§€ ì•ŠìŒ.|[Link](https://github.com/Ukbang/Aiffel_thon/blob/main/chatbot/Test/23-02-01_-100_test.ipynb)|
|2|skt/kogpt2-base-v2|5|Topic+kakao|Done|2023-02-01|Input|1.476 -> 1.343 -> 1.270 -> 1.203 -> 1.137|1.486 -> 1.445 -> 1.434 -> 1.441 -> 1.461|í˜„ì¬ê¹Œì§€ ê°€ì¥ Best|[Link](https://github.com/Ukbang/Aiffel_thon/blob/main/chatbot/Test/23-02-01_True_test.ipynb)|
|3|skt/kogpt2-base-v2|3|kakao Data|Done|2023-01-30|Input|2.330 -> 2.147 -> 2.084|1.765 -> 1.723 -> 1.704|ë¬¸ì¥ ìƒì„±ì„ eos token ë°–ì— ëª»í•¨.|[Link](https://github.com/Ukbang/Aiffel_thon/blob/main/chatbot/Test/Inference_code_label_True_len384.ipynb)|
|4|skt/kogpt2-base-v2|5|Topic+kakao|Done|2023-02-01|shift|2.140 -> 2.005 -> 1.931 -> 1.864 -> 1.794|2.298 -> 2.236 -> 2.215 -> 2.221 -> 2.246|í•™ìŠµì´ ì „í˜€ ë˜ì§€ ì•Šì•˜ìŒ. íê¸°|[Link]()|
|5|skt/kogpt2-base-v2|10|Topic+kakao|Done|2023-02-01|Input|1.483 -> 1.352 -> 1.275 -> 1.206 -> 1.135 -> 1.062 -> 0.986 -> 0.908 -> 0.830 -> 0.753|1.504 -> 1.469 -> 1.456 -> 1.463 -> 1.485 -> 1.517 -> 1.562 -> 1.616 -> 1.683 -> 1.759|5epoch ì´ìƒë¶€í„° í•™ìŠµì´ ì˜¤íˆë ¤ ì•ˆë¨. íê¸°|[Link]()|
|6|skt/kogpt2-base-v2|5|Topic+kakao|Done|2023-02-05|Input|1.479 -> 1.380 -> 1.330 -> 1.292 -> 1.260|1.401 -> 1.370 -> 1.357 -> 1.350 -> 1.346|ìì˜í•œ ì½”ë“œ ìˆ˜ì •í›„ í•™ìŠµí•˜ì˜€ìŒ. 2ë²ˆê³¼ ì„±ëŠ¥ì´ ë™ì¼í•¨.|[Link]()|
|7|skt/kogpt2-base-v2|5|Topic+kakao|Done|2023-02-05|-100|4.269 -> 3.869 -> 3.574 -> 3.296 -> 3.031|4.069 -> 3.997 -> 3.989 -> 4.032 -> 4.106|.....|[Link]()|
|8|skt/kogpt2-base-v2|5|Topic+kakao|Done|2023-02-05|-100+sys|3.826 -> |3.352 -> |.....|[Link]()|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|

---

### ì¶”ê°€ëœ ì„œë¹„ìŠ¤
<p align ="center"><img src="https://user-images.githubusercontent.com/112140135/216884826-5905e7cb-229a-4a53-becd-25508e40fd1d.png" width="600px" height="900px"></p>

---

### íšŒê³ 

- ì»¤ìŠ¤í…€ ë°ì´í„° ë¡œë” 
```python
token_ids ====>  tensor([[49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0]])
mask =====>  tensor([[2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0]])
label =====>  tensor([[-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100]])
(tensor([[49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0],
        [49186,     2,     4,  ...,     0,     0,     0]]), tensor([[2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0],
        [2, 2, 2,  ..., 0, 0, 0]]), tensor([[-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100],
        [-100, -100, -100,  ..., -100, -100, -100]]))
```
ì²« ì„¤ê³„ëŠ” ê¸¸ì´/í„´ ìˆ˜ê°€ ê¸´ ë°ì´í„°ëŠ” ë’·ë¶€ë¶„ì„ ìë¥´ê³  ì•ë¶€ë¶„ì„ ë‚¨ê¸°ëŠ” ë°©ì‹ìœ¼ë¡œ ì‹œë„í•¨

`<usr>ê°€ë‚˜ë‹¤<sys>ë¼ë§ˆë°”<usr>ì‚¬ì•„ì(ê¸¸ì´/í„´ìˆ˜ì´ˆê³¼)<sys>ì°¨ì¹´íƒ€` :  
`<usr>ê°€ë‚˜ë‹¤<sys>ë¼ë§ˆë°”` -- ìë¥¼ ë•ŒëŠ” ë¬´ì¡°ê±´ sys ëŒ€í™”ë¡œ ì¢…ë£Œ

```python
for i in tqdm(range(len(self.data))):
            hists = []
            dials = self.data[i]
            
            for u, utter in enumerate(dials):
                if u % 2 == 0:
                    hists.append(self.usr_token + utter)  # Speaker 1: User
                else:
                    hists.append(self.sys_token + utter)  # Speaker 2: System
            
            max_turn = max(len(hists), self.max_turns) # max_turnsì„ ë„˜ìœ¼ë©´ post
            if max_turn % 2 != 0: max_turn -= 1 # user ëŒ€í™”ë¡œ ëë‚¨ ë°©ì§€
                
            for f in range(max_turn, 1, -2):
                contexts = hists[:f]
                if sum([len(l) for l in contexts]) > self.max_len-2: continue # bos_tokenì™€ eos_token í† í°ì„ ì¶”ê°€í•˜ê¸° ìœ„í•´ -2
                contexts[0] = self.bos_token + contexts[0]
                contexts[-1] = contexts[-1] + self.eos_token
                contexts = [tokenizer.encode(ctx) for ctx in contexts]
                
                token_type_id = [[ctx[0]] * len(ctx) if c != 0 else [ctx[1]] * len(ctx) for c, ctx in enumerate(contexts)]
                label = [[-100] * len(ctx) if c != len(contexts)-1 else [-100] + ctx[1:] for c, ctx in enumerate(contexts)]
                
                input_id = list(chain.from_iterable(contexts))
                token_type_id = list(chain.from_iterable(token_type_id))
                label = list(chain.from_iterable(label))
                
                assert len(input_id) == len(token_type_id) == len(label), "There is something wrong in dialogue process."
                
                input_id, token_type_id, label = self.make_padding(input_id, token_type_id, label)
                
                self.input_ids.append(input_id)
                self.token_type_ids.append(token_type_id)
                self.labels.append(label)
                
                break
```
breakì„ ë‚¨ê²¨ë†“ì„ ê²½ìš° ë°ì´í„° í•˜ë‚˜ë‹¹ ìµœëŒ€ í•œ ê°œì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì§€ë§Œ  
breakì„ ì œê±°í•  ê²½ìš° ê°€ëŠ¥í•œ ëª¨ë“  í„´ìˆ˜ë¥¼ ëª¨ë‘ ë°ì´í„°ë¡œ ì‚¬ìš©í•´ agumentationí•¨

  ex: max_turns = 6  
  `<usr>ê°€ë‚˜ë‹¤<sys>ë¼ë§ˆë°”<usr>ì‚¬ì•„ì<sys>ì°¨ì¹´íƒ€<usr>íŒŒí•˜ê°€<sys>ë‚˜ë‹¤ë¼<usr>ë§ˆë°”ì‚¬<sys>ì•„ìì°¨` =  
   `<usr>ê°€ë‚˜ë‹¤<sys>ë¼ë§ˆë°”<usr>ì‚¬ì•„ì<sys>ì°¨ì¹´íƒ€<usr>íŒŒí•˜ê°€<sys>ë‚˜ë‹¤ë¼`, -- 6í„´  
    `<usr>ê°€ë‚˜ë‹¤<sys>ë¼ë§ˆë°”<usr>ì‚¬ì•„ì<sys>ì°¨ì¹´íƒ€`, -- 4í„´  
     `<usr>ê°€ë‚˜ë‹¤<sys>ë¼ë§ˆë°”`... -- 2  

í•˜ì§€ë§Œ í•™ìŠµ ì‹œê°„ë§Œ ëŠ˜ì–´ë‚  ë¿ í° ì„±ê³¼ëŠ” ì—†ì—ˆìŒ.

ì˜¤íˆë ¤ agumentationì„ í¬ê¸°í•˜ë©´ì„œ `__init__`ì´ ì•„ë‹Œ `get_item`ì—ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•  ê²½ìš° ì—°ì‚° ì†ë„ê°€ ë” ë¹ ë¦„

- class ChatBot
```python
class ChatBot:
    """
        __init__ : ì±—ë´‡ ëª¨ë¸ ìƒì„±
            Args : model, tokenizer, Config
        
        train : ëª¨ë¸ í•™ìŠµ ì§„í–‰
            Args : epochs, train_data, (validation_data), (save)
        
        load_model : ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
            Args : PATH
        
        save_model : ëª¨ë¸ ì €ì¥í•˜ê¸°
            Args : PATH
        
        talk : ì±—ë´‡ ëŒ€í™”í•˜ê¸°
            ëŒ€í™” ì¢…ë£Œ ë©˜íŠ¸ : quit
    """
    
    def __init__(self, model, tokenizer, Config):
        """
            Args : model, tokenizer, Config
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = Config.device
        self.name = Config.model_name
        self.optim = torch.optim.Adam(model.parameters(), lr=Config.learning_rate)
        
#         self.user_token_id = tokenizer.get_vocab()[Config.usr_token]
#         self.bot_token_id = tokenizer.get_vocab()[Config.sys_token]
        self.user_token_id = tokenizer.PieceToId(Config.usr_token)
        self.bot_token_id = tokenizer.PieceToId(Config.sys_token)
        self.max_len = Config.max_len
        self.max_turns = Config.max_turns
        
        self.losses = []
        self.val_losses = []
    
    def train(self, epochs, train_data, validation_data=None, save=None):
        """
            epochs, train_data, validation_data=None, save=None
            save : epochë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ/íŒŒì¼ëª…
        """
        self.model.to(self.device)
        for epoch in range(epochs):
            self.model.train()
            print(f"\n Epoch {epoch+1}/{epochs}", sep="\n")
            starttime = time.time()
            batch_loss = []

            for i, batch in enumerate(train_data):
                input_ids, token_type_ids, labels = batch
                input_ids, token_type_ids, labels = \
                    input_ids.to(self.device), token_type_ids.to(self.device), labels.to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    token_type_ids = token_type_ids,
                    labels = labels
                )
                
                loss, logits = outputs[0], outputs[1]
                
                self.optim.zero_grad()
                loss.backward()
                self.optim.step()

                batch_loss.append(loss.item())
                
                print(self.status(i+1, len(train_data), time.time()-starttime, np.mean(batch_loss)), end='\r')

            self.losses.append(np.mean(batch_loss))
            
            if validation_data:
                val_loss = self.validation(validation_data)
                print(self.status(i+1, len(train_data), time.time()-starttime, np.mean(batch_loss)) + \
                      " | val_loss : %.6f"%(val_loss), end='\r')
                self.val_losses.append(val_loss)
            
            if save:
                PATH = f'{save}_epochs-{epoch+1}_loss-{np.mean(batch_loss)}.pth'
                torch.save(self.model.state_dict(), PATH)
                
    def validation(self, validation_data):
        self.model.eval()
        batch_loss = []
        
        with torch.no_grad():
            for i, batch in enumerate(validation_data):
                input_ids, token_type_ids, labels = batch
                input_ids, token_type_ids, labels = \
                    input_ids.to(self.device), token_type_ids.to(self.device), labels.to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    token_type_ids = token_type_ids,
                    labels = labels
                )
                
                loss, logits = outputs[0], outputs[1]
                batch_loss.append(loss.item())
            
            valid_loss = np.mean(batch_loss)
        
        return valid_loss

    @staticmethod
    def status(step, step_len, time, loss):
        return "step : %d/%d - %ds | loss : %.6f | %.2fit/s"%(
            step,
            step_len,
            int(time),
            loss,
            step/time
        )
    
    def load_model(self, PATH):
        """
            PATH : pth íŒŒì¼ì´ ì €ì¥ëœ ê²½ë¡œ
        """
        self.model.load_state_dict(torch.load(PATH))
        print("model loaded.")
    
    def save_model(self, PATH=None):
        """
            PATH : ì €ì¥í•  íŒŒì¼ ê²½ë¡œ/ì´ë¦„, ìƒëµì‹œ ëª¨ë¸ ì´ë¦„ê³¼ í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì§€ì •í•¨
        """
        if not PATH:
            name = self.name.replace("/", "-")
            PATH = f"./{name}_{time.strftime('%Y-%m-%d %H:%M:%S')}.pth"
        torch.save(self.model.state_dict(), PATH)
        print("model saved.")
      
  -- talk ë©”ì„œë“œ ìƒëµ
```
model.fit ì™¸ì—ë„ ì›í•˜ëŠ” ê¸°ëŠ¥ë“¤ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í—ˆê¹…í˜ì´ìŠ¤ì˜ ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ ê³µë¶€í•˜ê¸°ë³´ë‹¤ ì§ì ‘ classë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•¨.

í•„ìš”í–ˆë˜ ê¸°ëŠ¥ìœ¼ë¡œëŠ” ê°„ë‹¨í•œ í•™ìŠµ ìˆ˜í–‰ , ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë©”ì„œë“œ, ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° ë©”ì„œë“œ, loss ë¦¬ìŠ¤íŠ¸
      
- SentencePiece
```python
import pandas as pd
import sentencepiece as spm
from tqdm import tqdm

data = pd.read_csv('spacing_data.csv', encoding='utf-8')
conversations = data['conversation'].tolist()

def get_tokenizer(corpus, lang, vocab_size=52000, s_tokens=False):
    temp_file = f"./SentencePiece_{lang}.txt"
    
    with open(temp_file, "w", encoding='utf-8') as f:
        for i in tqdm(range(len(corpus))):
            f.write(corpus[i] + '\n')
    
    print("file saved..")
    
    prefix = f"SentencePiece_{lang}"
    
    spm.SentencePieceTrainer.Train(
        f"--input={temp_file} --model_prefix={prefix} --vocab_size={vocab_size}" + 
        " --model_type=bpe" +
        " --pad_id=0" + # pad (0)
        " --unk_id=1" + # unknown (1)
        " --bos_id=2" + # begin of sequence (2)
        " --eos_id=3"   # end of sequence (3)
    )
    print("train has been finish")

    tokenizer = spm.SentencePieceProcessor()
    tokenizer.load(f"{prefix}.model")
    print("model loaded..")
    
    if s_tokens:
        tokenizer.set_encode_extra_options("bos:eos")
    
    corpus = [tokenizer.EncodeAsIds(sentence) for sentence in corpus]
    print("tokenizing has been finish")
    
    return corpus, tokenizer

sentences, tokenizer = get_tokenizer(conversations, '249388')
# tokenizer = spm.SentencePieceProcessor()
# tokenizer.load("SentencePiece_02-04.model")

# src_corpus = [tokenizer.EncodeAsIds(sentence) for sentence in corpus['document']]
```

í—ˆê¹…í˜ì´ìŠ¤ì˜ í† í¬ë‚˜ì´ì €ëŠ” vocabularyê°€ ì •í•´ì ¸ ìˆì–´ì„œ ì»¤ìŠ¤í…€ ë°ì´í„°ì— ë§ëŠ” ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ê°€ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œ ì‹œë„í•´ë´„.

ê²°ê³¼ëŠ” 

```RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
```

ì˜¤ë¥˜ë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹¤í—˜ ì¢…ë£Œ.

---
### ì°¸ê³  ë¬¸í—Œ
1. [songys/AwesomeKorean_Data: í•œêµ­ì–´ ë°ì´í„° ì„¸íŠ¸ ë§í¬](https://github.com/songys/AwesomeKorean_Data)
2. [ììœ ëŒ€í™”í˜•ì‹ì˜ ìŒì„± ë°ì´í„°](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=109)
3. [STTëª¨ë¸ ë° TTSëª¨ë¸ ê°œë°œ](https://www.youtube.com/watch?v=WTul6LIjIBA)
4. [ì˜¨ë¼ì¸ êµ¬ì–´ì²´ ë§ë­‰ì¹˜ ë°ì´í„°](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=625)
5. [ë²•ë¥  ì§€ì‹ ë² ì´ìŠ¤](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=99)
6. [íŒŒì´ì¬ìœ¼ë¡œ JSON íŒŒì¼ ë‹¤ë£¨ê¸°](https://www.youtube.com/watch?v=s9D-JIuaFqY&t=433s)
7. [korean SmileStyle Dataset](https://www.google.com/url?q=https://github.com/smilegate-ai/korean_smile_style_dataset&sa=D&source=docs&ust=1672048006339662&usg=AOvVaw2KWZl71R1gdPiznFcT1tkG)
8. [ì£¼ì œë³„ í…ìŠ¤íŠ¸ ì¼ìƒìƒí™œ ë°ì´í„°](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=543)
9. [í•œêµ­ì–´ ëŒ€í™” ìš”ì•½](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=117)
10. [í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸](https://huggingface.co/lcw99/ko-dialoGPT-korean-chit-chat)
11. [[NLP] ì–¸ì–´ëª¨ë¸ì˜ í‰ê°€ì§€í‘œ 'Perplexity' ê°œë… ë° ê³„ì‚°ë°©ë²•](https://heytech.tistory.com/344)
12. [ë¬´ìŠ¨ ëŒ€í™”ë“  í•  ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸ë¥¼ í–¥í•˜ì—¬](https://brunch.co.kr/@synabreu/35)
13. [PyTorch 2.0 ë¬´ì—‡ì´ ë‹¤ë¥¸ê°€?](https://blog.naver.com/october-eight/222948663006)
14. [Tensorflow_KoGPT2_Chabot](https://github.com/ukairia777/tensorflow-kogpt2-chatbot/blob/main/KoGPT2_Chatbot.ipynb)
15. [GPT-2 Fine Tuning ](https://blog.naver.com/ds_penaut/222699897818)
16. [CaFeCoKe/KoGPT2_Chatbot](https://github.com/CaFeCoKe/KoGPT2_Chatbot)
---
### íŒ€ì› ê¹ƒí—ˆë¸Œ ë§í¬

- [ë°©ìŠ¹ìš±](https://github.com/Ukbang)
- [êµ¬ë³¸íšŒ](https://github.com/HughBGrant) 
- [ì´íƒœí™˜](https://github.com/leetaehwan) 
- [ì¥ë¬¸ê·œ](https://github.com/MunGyuJang)

---
### Google Drive
- [êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§í¬](https://drive.google.com/drive/folders/13xvDPcMMqEe8cVTOg3VBjc0IgSjOAX9E)


 
